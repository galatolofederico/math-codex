<!doctype html><html lang="en"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no"/><meta name="description" content=""/><meta name="author" content=""/><title>Zero-shot natural language mathematical problem solving using Generative Pre-trained Transformers</title><script defer="defer" src="bundle.js"></script></head><body><nav class="navbar navbar-expand-lg navbar-dark bg-dark"><div class="container"><a class="navbar-brand" href="#">Math-Codex Zero-shot Learning</a> <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ms-auto mb-2 mb-lg-0"><li class="nav-item"><a class="nav-link" href="#">Paper</a></li><li class="nav-item"><a class="nav-link" href="https://github.com/galatolofederico/math-codex">Github</a></li><li class="nav-item"><a class="nav-link" href="https://mlpi.ing.unipi.it/">MLPI</a></li></ul></div></div></nav><div class="container"><div class="text-center mt-5"><h1>Zero-shot natural language mathematical problem solving<br/>using Generative Pre-trained Transformers</h1><p class="lead">Federico A. Galatolo, Mario G.C.A. Cimino, Gigliola Vaglini</p><p>Mathematics is an effective testbed for measuring the problem-solving ability of machine learning models. The current benchmark for deep learning-based solutions is grade school math problems: given a natural language description of a problem, the task is to analyse the problem, exploit heuristics generated from a very large set of solved examples, and then generate an answer. In this paper, a descendant of the third generation of Generative Pre-trained Transformer Networks (GPT-3) is used to develop a zero-shot learning approach, to solve this problem. The proposed approach shows that coding based problem-solving is more effective than the natural language reasoning based one. Specifically, the architectural solution is built upon OpenAI Codex, a descendant of GPT-3 for programming tasks, trained on public GitHub repositories, the worldâ€™s largest source code hosting service. Experimental results clearly show the potential of the approach: by exploiting the Python as programming language, proposed pipeline achieves the 18.63% solve rate against the 6.82% of GPT-3. Finally, by using a fine-tuned verifier, the correctness of the answer can be ranked at runtime, and then improved by generating a predefined number of trials. With this approach, for 10 trials and an ideal verifier, the proposed pipeline achieves 54.20% solve rate.</p></div></div><div class="container"><div class="text-center mt-5"><p>In the following table the outputs of the proposed pipeline and GPT-3 are presented for the whole GSM8K test set</p></div></div><div class="container container-giga"><div class="mt-5"><div id="table"></div></div></div><script src="./bundle.js"></script></body></html>